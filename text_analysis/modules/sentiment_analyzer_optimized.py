#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆæƒ…æ„Ÿåˆ†ææ¨¡å—
æ”¯æŒæœ¬åœ°è¯å…¸å’Œé˜¿é‡Œäº‘APIä¸¤ç§åˆ†ææ–¹å¼
ç»Ÿä¸€æ‰§è¡Œå…¥å£ï¼Œæ”¯æŒè§†é¢‘IDåˆ†æ
"""

import os
import sys
import re
import json
import time
import argparse
import logging
from datetime import datetime
from typing import Dict, List, Union, Optional
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

# å¯¼å…¥PROJECT_ROOT
from text_analysis.core.data_paths import PROJECT_ROOT

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from config.db_config import get_db_conn

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    logger.warning("python-dotenvæœªå®‰è£…ï¼Œæ— æ³•è‡ªåŠ¨åŠ è½½.envæ–‡ä»¶")

class DictionaryAnalyzer:
    """æœ¬åœ°è¯å…¸æƒ…æ„Ÿåˆ†æå™¨"""
    
    def __init__(self):
        self.sentiment_dict = self._load_sentiment_dict()
        self.negation_words = {'ä¸', 'æ²¡', 'æ— ', 'é', 'æœª', 'åˆ«', 'è«', 'å‹¿', 'æ¯‹', 'å¼—', 'å¦', 'å'}
        self.intensifier_words = {
            'éå¸¸': 2.0, 'ç‰¹åˆ«': 2.0, 'æå…¶': 2.0, 'ååˆ†': 2.0, 'å¾ˆ': 1.5, 'æŒº': 1.3,
            'æ¯”è¾ƒ': 1.2, 'æœ‰ç‚¹': 0.8, 'ç¨å¾®': 0.7, 'ç•¥å¾®': 0.7, 'å¤ª': 1.8, 'çœŸ': 1.5,
            'ç¡®å®': 1.3, 'çœŸçš„': 1.5, 'ç»å¯¹': 2.0, 'å®Œå…¨': 2.0,
        }
    
    def _load_sentiment_dict(self) -> Dict[str, float]:
        """åŠ è½½æƒ…æ„Ÿè¯å…¸"""
        return {
            # æ­£å‘æƒ…æ„Ÿè¯
            'å¥½': 1.0, 'æ£’': 1.0, 'èµ': 1.0, 'ä¼˜ç§€': 1.0, 'å®Œç¾': 1.0, 'ç²¾å½©': 1.0,
            'å–œæ¬¢': 1.0, 'çˆ±': 1.0, 'æ”¯æŒ': 1.0, 'æ¨è': 1.0, 'æ»¡æ„': 1.0, 'å¼€å¿ƒ': 1.0,
            'é«˜å…´': 1.0, 'å¿«ä¹': 1.0, 'å…´å¥‹': 1.0, 'æ¿€åŠ¨': 1.0, 'æ„ŸåŠ¨': 1.0, 'æ¸©æš–': 1.0,
            'ç¾å¥½': 1.0, 'æ¼‚äº®': 1.0, 'å¸…æ°”': 1.0, 'å¯çˆ±': 1.0, 'æœ‰è¶£': 1.0, 'æç¬‘': 1.0,
            'å‰å®³': 1.0, 'å¼ºå¤§': 1.0, 'ä¸“ä¸š': 1.0, 'é«˜è´¨é‡': 1.0, 'è¶…èµ': 1.5, 'å¤ªæ£’äº†': 1.5,
            '666': 1.0, 'ç‰›': 1.0, 'ç¥': 1.0, 'ç»äº†': 1.5, 'æ— æ•Œ': 1.5, 'çˆ†èµ': 1.5,
            'åŠ æ²¹': 0.8, 'å¥½æ ·': 0.8, 'æ£’æ£’': 1.0, 'æ£’æ£’å“’': 1.2, 'æ£’æäº†': 1.3,
            'å¤ªæ£’': 1.2, 'å¾ˆæ£’': 1.1, 'éå¸¸å¥½': 1.2, 'ç‰¹åˆ«å¥½': 1.2, 'è¶…çº§æ£’': 1.4,
            'å¤ªèµ': 1.2, 'å¾ˆèµ': 1.1, 'è¶…èµ': 1.3, 'èµèµ': 1.0, 'èµèµèµ': 1.2,
            'å–œæ¬¢': 1.0, 'å¾ˆå–œæ¬¢': 1.2, 'è¶…å–œæ¬¢': 1.3, 'çˆ±äº†': 1.1, 'çˆ±äº†çˆ±äº†': 1.3,
            
            # è´Ÿå‘æƒ…æ„Ÿè¯
            'å·®': -1.0, 'çƒ‚': -1.0, 'åƒåœ¾': -1.0, 'ç³Ÿç³•': -1.0, 'æ¶å¿ƒ': -1.0, 'è®¨åŒ': -1.0,
            'æ¨': -1.0, 'çƒ¦': -1.0, 'ç”Ÿæ°”': -1.0, 'æ„¤æ€’': -1.0, 'å¤±æœ›': -1.0, 'ä¼¤å¿ƒ': -1.0,
            'éš¾è¿‡': -1.0, 'ç—›è‹¦': -1.0, 'ç»æœ›': -1.0, 'å´©æºƒ': -1.0, 'æ— è¯­': -1.0, 'å‘': -1.0,
            'éª—': -1.0, 'å‡': -1.0, 'æ°´': -1.0, 'æ— èŠ': -1.0, 'æ²¡æ„æ€': -1.0, 'æµªè´¹æ—¶é—´': -1.0,
            'å¤ªå·®äº†': -1.5, 'å¤ªçƒ‚äº†': -1.5, 'å¤ªæ¶å¿ƒäº†': -1.5, 'å¤ªå‘äº†': -1.5, 'å¤ªå‡äº†': -1.5,
            'å¤ªæ— èŠäº†': -1.5, 'å¤ªå¤±æœ›äº†': -1.5, 'å¤ªä¼¤å¿ƒäº†': -1.5, 'å¤ªç—›è‹¦äº†': -1.5,
            'å¾ˆå·®': -1.1, 'éå¸¸å·®': -1.3, 'ç‰¹åˆ«å·®': -1.3, 'è¶…çº§å·®': -1.4,
            'å¤ªçƒ‚': -1.2, 'å¾ˆçƒ‚': -1.1, 'è¶…çƒ‚': -1.3, 'çƒ‚é€äº†': -1.4,
            'è®¨åŒ': -1.0, 'å¾ˆè®¨åŒ': -1.2, 'è¶…è®¨åŒ': -1.3, 'å¤ªè®¨åŒ': -1.2,
            'å¤±æœ›': -1.0, 'å¾ˆå¤±æœ›': -1.2, 'å¤ªå¤±æœ›': -1.3, 'éå¸¸å¤±æœ›': -1.4,
            
            # ä¸­æ€§æƒ…æ„Ÿè¯
            'ä¸€èˆ¬': 0.0, 'è¿˜è¡Œ': 0.0, 'å‡‘åˆ': 0.0, 'æ™®é€š': 0.0, 'æ­£å¸¸': 0.0, 'æ ‡å‡†': 0.0,
            'å¯ä»¥': 0.0, 'ä¸é”™': 0.0, 'è¿˜å¥½': 0.0, 'é©¬é©¬è™è™': 0.0, 'è¿‡å¾—å»': 0.0,
            'è¿˜è¡Œ': 0.0, 'å‡‘åˆ': 0.0, 'ä¸€èˆ¬èˆ¬': 0.0, 'é©¬é©¬è™è™': 0.0, 'è¿‡å¾—å»': 0.0,
        }
    
    def analyze_text(self, text: str) -> Dict[str, Union[str, float]]:
        """åˆ†ææ–‡æœ¬æƒ…æ„Ÿ"""
        if not text or not text.strip():
            return {'sentiment': 'neutral', 'score': 0.0, 'confidence': 0.0}
        
        # æ–‡æœ¬é¢„å¤„ç†
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', text)
        words = text.split()
        
        if not words:
            return {'sentiment': 'neutral', 'score': 0.0, 'confidence': 0.0}
        
        total_score = 0.0
        word_count = 0
        
        for i, word in enumerate(words):
            word_score = 0.0
            
            # æ£€æŸ¥æƒ…æ„Ÿè¯å…¸
            if word in self.sentiment_dict:
                word_score = self.sentiment_dict[word]
            
            # æ£€æŸ¥ç¨‹åº¦å‰¯è¯
            if i > 0 and words[i-1] in self.intensifier_words:
                word_score *= self.intensifier_words[words[i-1]]
            
            # æ£€æŸ¥å¦å®šè¯
            if i > 0 and words[i-1] in self.negation_words:
                word_score *= -1
            
            total_score += word_score
            word_count += 1
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        if word_count > 0:
            score = total_score / word_count
        else:
            score = 0.0
        
        # ç¡®å®šæƒ…æ„Ÿææ€§
        if score >= 0.3:
            sentiment = 'positive'
        elif score <= -0.3:
            sentiment = 'negative'
        else:
            sentiment = 'neutral'
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = min(abs(score) * 2, 0.9) if abs(score) > 0 else 0.3
        
        return {
            'sentiment': sentiment,
            'score': score,
            'confidence': confidence,
            'method': 'dictionary'
        }

class AliyunAnalyzer:
    """é˜¿é‡Œäº‘NLPæƒ…æ„Ÿåˆ†æå™¨"""
    
    def __init__(self):
        self.access_key_id = os.getenv('NLP_AK_ENV')
        self.access_key_secret = os.getenv('NLP_SK_ENV')
        self.region_id = os.getenv('NLP_REGION_ENV', 'cn-hangzhou')
        self.endpoint = f"https://nlp.{self.region_id}.aliyuncs.com"
        
        if not self.access_key_id or not self.access_key_secret:
            raise ValueError("é˜¿é‡Œäº‘AccessKeyæœªé…ç½®ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡NLP_AK_ENVå’ŒNLP_SK_ENV")
    
    def analyze_text(self, text: str) -> Dict[str, Union[str, float]]:
        """åˆ†ææ–‡æœ¬æƒ…æ„Ÿ"""
        try:
            # ä¼˜å…ˆä½¿ç”¨SDK
            return self._analyze_with_sdk(text)
        except Exception as e:
            logger.warning(f"SDKåˆ†æå¤±è´¥ï¼Œå°è¯•HTTPè¯·æ±‚: {e}")
            try:
                return self._analyze_with_http(text)
            except Exception as e2:
                logger.error(f"HTTPè¯·æ±‚ä¹Ÿå¤±è´¥: {e2}")
                # è¿”å›é»˜è®¤ç»“æœ
                return {
                    'sentiment': 'neutral',
                    'score': 0.0,
                    'confidence': 0.0,
                    'error': f"APIè¿æ¥å¤±è´¥: {e2}",
                    'method': 'aliyun'
                }
    
    def _analyze_with_sdk(self, text: str) -> Dict[str, Union[str, float]]:
        """ä½¿ç”¨SDKåˆ†æ"""
        try:
            from aliyunsdkcore.client import AcsClient
            from aliyunsdkcore.request import CommonRequest
            
            # åˆ›å»ºAcsClientå®ä¾‹
            client = AcsClient(
                self.access_key_id,
                self.access_key_secret,
                self.region_id
            )
            
            # ä½¿ç”¨CommonRequest
            request = CommonRequest()
            request.set_domain('alinlp.cn-hangzhou.aliyuncs.com')
            request.set_version('2020-06-29')
            request.set_action_name('GetSaChGeneral')
            request.add_query_param('ServiceCode', 'alinlp')
            request.add_query_param('Text', text)
            
            # å‘é€è¯·æ±‚
            response = client.do_action_with_exception(request)
            result = json.loads(response)
            
            return self._parse_response(result)
            
        except ImportError:
            raise Exception("é˜¿é‡Œäº‘SDKæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install aliyun-python-sdk-core")
        except Exception as e:
            raise e
    
    def _analyze_with_http(self, text: str) -> Dict[str, Union[str, float]]:
        """ä½¿ç”¨HTTPè¯·æ±‚åˆ†æ"""
        import requests
        import hashlib
        import hmac
        import base64
        
        params = {
            'Action': 'SentimentAnalysis',
            'Version': '2018-04-08',
            'Format': 'JSON',
            'Timestamp': datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ'),
            'SignatureMethod': 'HMAC-SHA1',
            'SignatureVersion': '1.0',
            'SignatureNonce': str(int(time.time() * 1000)),
            'AccessKeyId': self.access_key_id,
            'Text': text,
        }
        
        # ç”Ÿæˆç­¾å
        signature = self._generate_signature('POST', '/', params)
        params['Signature'] = signature
        
        try:
            response = requests.post(self.endpoint, data=params, timeout=30)
            response.raise_for_status()
            result = response.json()
            return self._parse_response(result)
        except Exception as e:
            raise e
    
    def _generate_signature(self, method: str, path: str, params: Dict) -> str:
        """ç”Ÿæˆç­¾å"""
        canonicalized_query_string = "&".join([f"{k}={v}" for k, v in sorted(params.items())])
        string_to_sign = f"{method}\n{path}\n{canonicalized_query_string}\n"
        
        signature = hmac.new(
            self.access_key_secret.encode('utf-8'),
            string_to_sign.encode('utf-8'),
            hashlib.sha1
        ).digest()
        
        return base64.b64encode(signature).decode('utf-8')
    
    def _parse_response(self, result: Dict) -> Dict[str, Union[str, float]]:
        """è§£æå“åº”"""
        try:
            # è§£æDataå­—æ®µ
            data_str = result.get('Data', '{}')
            if isinstance(data_str, str):
                data = json.loads(data_str)
            else:
                data = data_str
            
            # è·å–ç»“æœ
            result_data = data.get('result', {})
            sentiment_zh = result_data.get('sentiment', '')
            positive_prob = float(result_data.get('positive_prob', 0))
            negative_prob = float(result_data.get('negative_prob', 0))
            neutral_prob = float(result_data.get('neutral_prob', 0))
            
            # æ˜ å°„æƒ…æ„Ÿ
            sentiment_map = {
                'positive': 'positive', 'negative': 'negative', 'neutral': 'neutral',
                'æ­£å‘': 'positive', 'è´Ÿå‘': 'negative', 'ä¸­æ€§': 'neutral',
                'æ­£é¢': 'positive', 'è´Ÿé¢': 'negative',
            }
            
            sentiment = sentiment_map.get(sentiment_zh.lower(), 'neutral')
            
            # è®¡ç®—åˆ†æ•°å’Œç½®ä¿¡åº¦
            if sentiment == 'positive':
                score = positive_prob
                confidence = positive_prob
            elif sentiment == 'negative':
                score = -negative_prob
                confidence = negative_prob
            else:
                score = 0.0
                confidence = neutral_prob
            
            return {
                'sentiment': sentiment,
                'score': score,
                'confidence': confidence,
                'positive_prob': positive_prob,
                'negative_prob': negative_prob,
                'neutral_prob': neutral_prob,
                'method': 'aliyun'
            }
        except Exception as e:
            raise e

class SentimentAnalyzer:
    """ç»Ÿä¸€æƒ…æ„Ÿåˆ†æå™¨"""
    
    def __init__(self, analyzer_type: str = "dictionary"):
        """
        åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨
        
        Args:
            analyzer_type: åˆ†æå™¨ç±»å‹ ("dictionary" æˆ– "aliyun")
        """
        self.analyzer_type = analyzer_type
        
        if analyzer_type == "dictionary":
            self.analyzer = DictionaryAnalyzer()
        elif analyzer_type == "aliyun":
            self.analyzer = AliyunAnalyzer()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†æå™¨ç±»å‹: {analyzer_type}")
        
        self.stats = {
            'total_analyzed': 0,
            'positive_count': 0,
            'negative_count': 0,
            'neutral_count': 0,
            'total_confidence': 0.0,
            'total_score': 0.0,
            'errors': 0
        }
    
    def analyze_text(self, text: str) -> Dict[str, Union[str, float]]:
        """åˆ†æå•ä¸ªæ–‡æœ¬"""
        try:
            result = self.analyzer.analyze_text(text)
            self._update_stats(result)
            return result
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            self.stats['errors'] += 1
            return {
                'sentiment': 'neutral',
                'score': 0.0,
                'confidence': 0.0,
                'error': str(e),
                'method': self.analyzer_type
            }
    
    def analyze_comments(self, conn, video_id: Optional[str] = None, limit: Optional[int] = None) -> pd.DataFrame:
        """
        åˆ†æè¯„è®ºæ•°æ®
        
        Args:
            conn: æ•°æ®åº“è¿æ¥
            video_id: è§†é¢‘IDï¼Œå¦‚æœä¸ºNoneåˆ™åˆ†ææ‰€æœ‰è¯„è®º
            limit: é™åˆ¶åˆ†ææ•°é‡ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ†ææ‰€æœ‰è¯„è®º
        """
        print("=== ä»æ•°æ®åº“åŠ è½½è¯„è®ºæ•°æ® ===")
        
        if video_id:
            # åˆ†ææŒ‡å®šè§†é¢‘çš„è¯„è®º
            sql = """
            SELECT comment_id, content, create_time, like_count, aweme_id
            FROM douyin_aweme_comment
            WHERE content IS NOT NULL AND LENGTH(content) > 5 AND aweme_id = %s
            ORDER BY create_time DESC
            """
            params = [video_id]
            print(f"åˆ†æè§†é¢‘ {video_id} çš„è¯„è®º...")
        else:
            # åˆ†ææ‰€æœ‰è¯„è®º
            sql = """
            SELECT comment_id, content, create_time, like_count, aweme_id
            FROM douyin_aweme_comment
            WHERE content IS NOT NULL AND LENGTH(content) > 5
            ORDER BY create_time DESC
            """
            params = []
            print("åˆ†ææ‰€æœ‰è¯„è®º...")
        
        if limit:
            sql += f" LIMIT {limit}"
            print(f"é™åˆ¶åˆ†ææ•°é‡: {limit}")
        
        try:
            df = pd.read_sql_query(sql, conn, params=params)
            print(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡è¯„è®º")
            
            if df.empty:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è¯„è®º")
                return df
            
            # è¿›è¡Œæƒ…æ„Ÿåˆ†æ
            print("=== å¼€å§‹æƒ…æ„Ÿåˆ†æ ===")
            sentiments = []
            scores = []
            confidences = []
            
            for idx, row in df.iterrows():
                if idx % 50 == 0 and idx > 0:
                    print(f"æ­£åœ¨åˆ†æç¬¬ {idx+1}/{len(df)} æ¡è¯„è®º...")
                
                result = self.analyze_text(row['content'])
                sentiments.append(result['sentiment'])
                scores.append(result['score'])
                confidences.append(result['confidence'])
            
            # æ·»åŠ ç»“æœåˆ°DataFrame
            df['sentiment'] = sentiments
            df['sentiment_score'] = scores
            df['sentiment_confidence'] = confidences
            
            print("âœ… æƒ…æ„Ÿåˆ†æå®Œæˆ")
            return df
            
        except Exception as e:
            print(f"âŒ æ•°æ®åº“åˆ†æå¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _update_stats(self, result: Dict[str, Union[str, float]]):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['total_analyzed'] += 1
        self.stats['total_confidence'] += result.get('confidence', 0.0)
        self.stats['total_score'] += result.get('score', 0.0)
        
        sentiment = result.get('sentiment', 'neutral')
        if sentiment == 'positive':
            self.stats['positive_count'] += 1
        elif sentiment == 'negative':
            self.stats['negative_count'] += 1
        else:
            self.stats['neutral_count'] += 1
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        if stats['total_analyzed'] > 0:
            stats['avg_confidence'] = stats['total_confidence'] / stats['total_analyzed']
            stats['avg_score'] = stats['total_score'] / stats['total_analyzed']
        else:
            stats['avg_confidence'] = 0.0
            stats['avg_score'] = 0.0
        return stats
    
    def save_results(self, df: pd.DataFrame, output_dir: str = None):
        """ä¿å­˜åˆ†æç»“æœ"""
        if output_dir is None:
            output_dir = os.path.join(PROJECT_ROOT, 'data', 'results')
        
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        base_filename = f"sentiment_analysis_{self.analyzer_type}_{timestamp}"
        
        # ä¿å­˜ä¸ºCSV
        csv_file = os.path.join(output_dir, f"{base_filename}.csv")
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {csv_file}")
        
        # ä¿å­˜ä¸ºJSON
        json_file = os.path.join(output_dir, f"{base_filename}.json")
        results = {
            'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'analyzer_type': self.analyzer_type,
            'total_comments': len(df),
            'stats': self.get_stats(),
            'data': df.to_dict('records')
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {json_file}")
        
        return csv_file, json_file
    
    def generate_report(self, df: pd.DataFrame, output_dir: str = None):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("=== ç”Ÿæˆåˆ†ææŠ¥å‘Š ===")
        
        if output_dir is None:
            output_dir = os.path.join(PROJECT_ROOT, 'data', 'reports')
        
        os.makedirs(output_dir, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = self.get_stats()
        sentiment_counts = df['sentiment'].value_counts()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'analyzer_type': self.analyzer_type,
            'summary': {
                'total_comments': len(df),
                'positive_count': int(sentiment_counts.get('positive', 0)),
                'negative_count': int(sentiment_counts.get('negative', 0)),
                'neutral_count': int(sentiment_counts.get('neutral', 0)),
                'avg_confidence': float(stats['avg_confidence']),
                'avg_score': float(stats['avg_score'])
            },
            'sentiment_distribution': sentiment_counts.to_dict(),
            'top_positive_comments': df[df['sentiment'] == 'positive'].nlargest(5, 'sentiment_score')[['content', 'sentiment_score']].to_dict('records'),
            'top_negative_comments': df[df['sentiment'] == 'negative'].nsmallest(5, 'sentiment_score')[['content', 'sentiment_score']].to_dict('records'),
        }
        
        # ä¿å­˜æŠ¥å‘Š
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = os.path.join(output_dir, f"sentiment_analysis_report_{self.analyzer_type}_{timestamp}.json")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # æ‰“å°æŠ¥å‘Šæ‘˜è¦
        print("\n=== åˆ†ææŠ¥å‘Šæ‘˜è¦ ===")
        print(f"åˆ†ææ—¶é—´: {report['analysis_time']}")
        print(f"åˆ†æå™¨ç±»å‹: {report['analyzer_type']}")
        print(f"æ€»è¯„è®ºæ•°: {report['summary']['total_comments']:,}")
        print(f"æ­£å‘è¯„è®º: {report['summary']['positive_count']:,} ({report['summary']['positive_count']/report['summary']['total_comments']*100:.1f}%)")
        print(f"è´Ÿå‘è¯„è®º: {report['summary']['negative_count']:,} ({report['summary']['negative_count']/report['summary']['total_comments']*100:.1f}%)")
        print(f"ä¸­æ€§è¯„è®º: {report['summary']['neutral_count']:,} ({report['summary']['neutral_count']/report['summary']['total_comments']*100:.1f}%)")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {report['summary']['avg_confidence']:.3f}")
        print(f"å¹³å‡æƒ…æ„Ÿåˆ†æ•°: {report['summary']['avg_score']:.3f}")
        
        return report_file
    
    def create_visualizations(self, df: pd.DataFrame, output_dir: str = None):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        print("=== åˆ›å»ºå¯è§†åŒ–å›¾è¡¨ ===")
        
        if output_dir is None:
            output_dir = os.path.join(PROJECT_ROOT, 'data', 'visualizations')
        
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('æƒ…æ„Ÿåˆ†æç»“æœå¯è§†åŒ–', fontsize=16, fontweight='bold')
        
        # 1. æƒ…æ„Ÿåˆ†å¸ƒé¥¼å›¾
        ax1 = axes[0, 0]
        sentiment_counts = df['sentiment'].value_counts()
        colors = ['lightgreen', 'lightcoral', 'lightblue']
        wedges, texts, autotexts = ax1.pie(sentiment_counts.values, labels=sentiment_counts.index, 
                                          autopct='%1.1f%%', colors=colors, startangle=90)
        ax1.set_title('æƒ…æ„Ÿåˆ†å¸ƒ')
        
        # 2. æƒ…æ„Ÿåˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾
        ax2 = axes[0, 1]
        ax2.hist(df['sentiment_score'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        ax2.set_xlabel('æƒ…æ„Ÿåˆ†æ•°')
        ax2.set_ylabel('é¢‘æ¬¡')
        ax2.set_title('æƒ…æ„Ÿåˆ†æ•°åˆ†å¸ƒ')
        ax2.grid(True, alpha=0.3)
        
        # 3. ç½®ä¿¡åº¦åˆ†å¸ƒ
        ax3 = axes[1, 0]
        ax3.hist(df['confidence'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')
        ax3.set_xlabel('ç½®ä¿¡åº¦')
        ax3.set_ylabel('é¢‘æ¬¡')
        ax3.set_title('ç½®ä¿¡åº¦åˆ†å¸ƒ')
        ax3.grid(True, alpha=0.3)
        
        # 4. æƒ…æ„Ÿåˆ†æ•°vsç½®ä¿¡åº¦æ•£ç‚¹å›¾
        ax4 = axes[1, 1]
        scatter = ax4.scatter(df['sentiment_score'], df['confidence'], 
                            c=df['sentiment_score'], cmap='RdYlGn', alpha=0.6)
        ax4.set_xlabel('æƒ…æ„Ÿåˆ†æ•°')
        ax4.set_ylabel('ç½®ä¿¡åº¦')
        ax4.set_title('æƒ…æ„Ÿåˆ†æ•° vs ç½®ä¿¡åº¦')
        ax4.grid(True, alpha=0.3)
        plt.colorbar(scatter, ax=ax4)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = os.path.join(output_dir, f'sentiment_analysis_visualization_{self.analyzer_type}_{timestamp}.png')
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {output_file}")
        
        # æ˜¾ç¤ºå›¾è¡¨
        plt.show()
        
        return output_file

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¼˜åŒ–ç‰ˆæƒ…æ„Ÿåˆ†æå·¥å…·")
    parser.add_argument('--type', choices=['local', 'aliyun'], 
                       default='local', help='åˆ†æå™¨ç±»å‹ï¼šlocal(æœ¬åœ°è¯å…¸) æˆ– aliyun(é˜¿é‡Œäº‘API)')
    parser.add_argument('--video-id', type=str, help='è§†é¢‘IDï¼Œå¦‚æœä¸æŒ‡å®šåˆ™åˆ†ææ‰€æœ‰è¯„è®º')
    parser.add_argument('--limit', type=int, help='é™åˆ¶åˆ†ææ•°é‡')
    parser.add_argument('--use-cleaned-data', action='store_true', help='ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®æ–‡ä»¶')
    parser.add_argument('--cleaned-data-path', type=str, help='æ¸…æ´—æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼Œåªåˆ†æå°‘é‡æ•°æ®')
    
    args = parser.parse_args()
    
    # æµ‹è¯•æ¨¡å¼è®¾ç½®
    if args.test:
        if not args.limit:
            args.limit = 10
        print("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªåˆ†æå°‘é‡æ•°æ®")
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("=== ä¼˜åŒ–ç‰ˆæƒ…æ„Ÿåˆ†æå·¥å…· ===")
    print(f"åˆ†æå™¨ç±»å‹: {args.type}")
    if args.video_id:
        print(f"è§†é¢‘ID: {args.video_id}")
    if args.limit:
        print(f"é™åˆ¶æ•°é‡: {args.limit}")
    if args.use_cleaned_data:
        print("ä½¿ç”¨æ¸…æ´—æ•°æ®")
    print("=" * 30)
    
    # æ£€æŸ¥é˜¿é‡Œäº‘APIé…ç½®
    if args.type == 'aliyun':
        access_key_id = os.getenv('NLP_AK_ENV')
        access_key_secret = os.getenv('NLP_SK_ENV')
        if not access_key_id or not access_key_secret:
            print("âŒ é˜¿é‡Œäº‘APIå¯†é’¥æœªé…ç½®")
            print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼š")
            print("  - NLP_AK_ENV: é˜¿é‡Œäº‘AccessKey ID")
            print("  - NLP_SK_ENV: é˜¿é‡Œäº‘AccessKey Secret")
            print("  - NLP_REGION_ENV: é˜¿é‡Œäº‘åŒºåŸŸID (å¯é€‰ï¼Œé»˜è®¤ä¸ºcn-hangzhou)")
            return
        print("âœ… é˜¿é‡Œäº‘APIç¯å¢ƒå˜é‡å·²é…ç½®")
    
    # åˆ›å»ºåˆ†æå™¨
    try:
        # æ˜ å°„å‚æ•°ç±»å‹
        analyzer_type = "dictionary" if args.type == "local" else "aliyun"
        analyzer = SentimentAnalyzer(analyzer_type)
        print("âœ… æƒ…æ„Ÿåˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # åŠ è½½æ•°æ®
    if args.use_cleaned_data:
        # ä»æ¸…æ´—æ•°æ®æ–‡ä»¶åŠ è½½
        try:
            if args.cleaned_data_path:
                cleaned_data_path = args.cleaned_data_path
            else:
                cleaned_data_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'douyin_comments_processed.json')
            
            if not os.path.exists(cleaned_data_path):
                print(f"âŒ æ¸…æ´—æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {cleaned_data_path}")
                return
            
            with open(cleaned_data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            df = pd.DataFrame(data)
            print(f"âœ… æˆåŠŸåŠ è½½æ¸…æ´—æ•°æ®: {len(df)} æ¡è®°å½•")
            
            # é™åˆ¶æ•°æ®é‡
            if args.limit and len(df) > args.limit:
                df = df.head(args.limit)
                print(f"âœ… é™åˆ¶æ•°æ®é‡: {len(df)} æ¡è®°å½•")
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ¸…æ´—æ•°æ®å¤±è´¥: {e}")
            return
    else:
        # ä»æ•°æ®åº“åŠ è½½
        try:
            conn = get_db_conn()
            print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return
        
        try:
            # åˆ†æè¯„è®º
            df = analyzer.analyze_comments(conn, args.video_id, args.limit)
            
            if df.empty:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°è¯„è®ºæ•°æ®")
                return
        except Exception as e:
            print(f"âŒ ä»æ•°æ®åº“åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return
    
    # æ‰§è¡Œåˆ†æ
    try:
        # å¯¹æ•°æ®è¿›è¡Œæƒ…æ„Ÿåˆ†æ
        print("=== å¼€å§‹æƒ…æ„Ÿåˆ†æ ===")
        results = []
        for idx, row in df.iterrows():
            result = analyzer.analyze_text(row['content'])
            results.append(result)
        
        # å°†ç»“æœæ·»åŠ åˆ°DataFrame
        df['sentiment'] = [r['sentiment'] for r in results]
        df['sentiment_score'] = [r['score'] for r in results]
        df['confidence'] = [r['confidence'] for r in results]
        
        print("âœ… æƒ…æ„Ÿåˆ†æå®Œæˆ")
        
        # ä¿å­˜ç»“æœ
        analyzer.save_results(df)
        
        # ç”ŸæˆæŠ¥å‘Š
        analyzer.generate_report(df)
        
        # åˆ›å»ºå¯è§†åŒ–
        analyzer.create_visualizations(df)
        
        print("\nâœ… æƒ…æ„Ÿåˆ†æå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        if not args.use_cleaned_data and 'conn' in locals():
            conn.close()
            print("âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")

if __name__ == "__main__":
    main()
