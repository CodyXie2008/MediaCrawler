#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠ–éŸ³è¯„è®ºæ•°æ®å‡†å¤‡ä¸æ¸…æ´—å®Œæ•´æµç¨‹
åŒ…å«ï¼šæ•°æ®å‡†å¤‡é˜¶æ®µ + æ•°æ®æ¸…æ´—é˜¶æ®µ
"""

import sys
import os
import re
import json
from datetime import datetime, timedelta
sys.path.append('..')

import pandas as pd
import jieba
from config.db_config import get_db_conn

class CommentDataProcessor:
    def __init__(self):
        self.stop_words = self.load_stop_words()
        
    def load_stop_words(self):
        """åŠ è½½åœç”¨è¯"""
        stop_words = set()
        try:
            # å°è¯•åŠ è½½é¡¹ç›®è‡ªå¸¦çš„åœç”¨è¯æ–‡ä»¶
            stop_words_file = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'docs', 'hit_stopwords.txt')
            if os.path.exists(stop_words_file):
                with open(stop_words_file, 'r', encoding='utf-8') as f:
                    stop_words = set(line.strip() for line in f if line.strip())
            else:
                # ä½¿ç”¨åŸºç¡€åœç”¨è¯
                stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
        except Exception as e:
            print(f"åŠ è½½åœç”¨è¯å¤±è´¥: {e}")
            stop_words = set()
        return stop_words
    
    def is_spam_comment(self, content, user_signature=None):
        """åˆ¤æ–­æ˜¯å¦ä¸ºåƒåœ¾è¯„è®º"""
        if not content:
            return True
        
        # å†…å®¹é•¿åº¦æ£€æŸ¥
        if len(content.strip()) < 5:
            return True
        
        # ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹æ£€æŸ¥ - æé«˜é˜ˆå€¼ï¼Œé¿å…è¯¯è¿‡æ»¤è¡¨æƒ…ç¬¦å·
        special_char_ratio = len(re.findall(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', content)) / len(content)
        if special_char_ratio > 0.6:  # æé«˜åˆ°60%ï¼Œé¿å…è¯¯è¿‡æ»¤æ­£å¸¸è¡¨æƒ…ç¬¦å·
            return True
        
        # å¹¿å‘Šå…³é”®è¯æ£€æŸ¥
        ad_keywords = [
            'åŠ å¾®ä¿¡', 'åŠ qq', 'åŠ ç¾¤', 'ç§ä¿¡', 'è”ç³»æˆ‘', 'åˆä½œ', 'æ¨å¹¿', 'å¹¿å‘Š',
            'èµšé’±', 'å…¼èŒ', 'ä»£ç†', 'åŠ ç›Ÿ', 'æ‹›å•†', 'æŠ•èµ„', 'ç†è´¢', 'è´·æ¬¾',
            'å…è´¹é¢†å–', 'é™æ—¶ä¼˜æƒ ', 'ç‚¹å‡»é“¾æ¥', 'æ‰«ç å…³æ³¨', 'å…³æ³¨å…¬ä¼—å·'
        ]
        content_lower = content.lower()
        for keyword in ad_keywords:
            if keyword in content_lower:
                return True
        
        # é‡å¤å­—ç¬¦æ£€æŸ¥ - æé«˜é˜ˆå€¼
        if len(set(content)) < len(content) * 0.2:  # é™ä½åˆ°20%ï¼Œé¿å…è¯¯è¿‡æ»¤
            return True
        
        # ç”¨æˆ·ç­¾åæ£€æŸ¥
        if user_signature:
            if len(user_signature) > 100:
                return True
            if re.search(r'[0-9]{8,}', user_signature):
                return True
        
        return False
    
    def clean_text(self, text):
        """æ–‡æœ¬æ¸…æ´—"""
        if not text:
            return ""
        
        # 1. å»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        
        # 2. å»é™¤URL
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # 3. å»é™¤æ–¹æ‹¬å·å†…å®¹ï¼ˆå¦‚[666][æ¯”å¿ƒ]ç­‰ï¼‰
        text = re.sub(r'\[[^\]]*\]', '', text)
        
        # 4. å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()
        
        # 5. ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ä¸­æ–‡æ ‡ç‚¹ï¼Œå»é™¤å…¶ä»–ç¬¦å·
        # ä¸­æ–‡æ ‡ç‚¹èŒƒå›´ï¼š\u3000-\u303f (å…¨è§’å­—ç¬¦)
        # ä¸­æ–‡æ ‡ç‚¹èŒƒå›´ï¼š\uff00-\uffef (å…¨è§’å­—ç¬¦)
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\u3000-\u303f\uff00-\uffef]', '', text)
        
        return text
    
    def segment_text(self, text):
        """ä¸­æ–‡åˆ†è¯"""
        if not text:
            return []
        
        # ä½¿ç”¨jiebaåˆ†è¯
        words = jieba.lcut(text)
        
        # å»é™¤åœç”¨è¯
        words = [word for word in words if word not in self.stop_words and len(word.strip()) > 0]
        
        return words
    
    def get_raw_comments(self, conn, start_time=None, end_time=None):
        """è·å–åŸå§‹è¯„è®ºæ•°æ®ï¼ˆä¸è¿›è¡Œä»»ä½•è¿‡æ»¤ï¼‰"""
        base_sql = """
        SELECT 
            comment_id, aweme_id, parent_comment_id, content, create_time, 
            like_count, sub_comment_count, user_id, nickname, user_signature
        FROM douyin_aweme_comment
        WHERE content IS NOT NULL
        """
        
        # æ·»åŠ æ—¶é—´è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
        if start_time:
            start_timestamp = int(start_time.timestamp())
            base_sql += f" AND create_time >= {start_timestamp}"
        
        if end_time:
            end_timestamp = int(end_time.timestamp())
            base_sql += f" AND create_time <= {end_timestamp}"
        
        print(f"æ‰§è¡ŒSQLæŸ¥è¯¢: {base_sql}")
        
        # è¯»å–æ•°æ®
        df = pd.read_sql_query(base_sql, conn)
        print(f"åŸå§‹æ•°æ®é‡: {len(df)} æ¡")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        self.check_data_quality(df)
        
        return df
    
    def get_filtered_comments(self, conn, start_time=None, end_time=None, min_likes=0, min_sub_comments=0):
        """è·å–è¿‡æ»¤åçš„è¯„è®ºæ•°æ®ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ä½œä¸ºå¤‡é€‰ï¼‰"""
        base_sql = """
        SELECT 
            comment_id, aweme_id, parent_comment_id, content, create_time, 
            like_count, sub_comment_count, user_id, nickname, user_signature
        FROM douyin_aweme_comment
        WHERE content IS NOT NULL 
        AND CHAR_LENGTH(TRIM(content)) >= 5
        """
        
        # æ·»åŠ æ—¶é—´è¿‡æ»¤
        if start_time:
            start_timestamp = int(start_time.timestamp())
            base_sql += f" AND create_time >= {start_timestamp}"
        
        if end_time:
            end_timestamp = int(end_time.timestamp())
            base_sql += f" AND create_time <= {end_timestamp}"
        
        # æ·»åŠ çƒ­é—¨åº¦è¿‡æ»¤
        if min_likes > 0:
            base_sql += f" AND CAST(like_count AS UNSIGNED) >= {min_likes}"
        
        if min_sub_comments > 0:
            base_sql += f" AND CAST(sub_comment_count AS UNSIGNED) >= {min_sub_comments}"
        
        print(f"æ‰§è¡ŒSQLæŸ¥è¯¢: {base_sql}")
        
        # è¯»å–æ•°æ®
        df = pd.read_sql_query(base_sql, conn)
        print(f"åŸå§‹æ•°æ®é‡: {len(df)} æ¡")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        self.check_data_quality(df)
        
        # è¿‡æ»¤åƒåœ¾è¯„è®º
        df_clean = self.filter_spam_comments(df)
        
        return df_clean
    
    def check_data_quality(self, df):
        """æ•°æ®è´¨é‡æ£€æŸ¥"""
        print("\n=== æ•°æ®è´¨é‡æ£€æŸ¥ ===")
        print(f"ç©ºå†…å®¹æ•°é‡: {df['content'].isnull().sum()}")
        print(f"ç©ºè¯„è®ºIDæ•°é‡: {df['comment_id'].isnull().sum()}")
        print(f"ç©ºè§†é¢‘IDæ•°é‡: {df['aweme_id'].isnull().sum()}")
        print(f"ç©ºæ—¶é—´æˆ³æ•°é‡: {df['create_time'].isnull().sum()}")
    
    def filter_spam_comments(self, df):
        """è¿‡æ»¤åƒåœ¾è¯„è®º"""
        print("\n=== è¿‡æ»¤åƒåœ¾è¯„è®º ===")
        before_filter = len(df)
        
        df['is_spam'] = df.apply(
            lambda row: self.is_spam_comment(row['content'], row.get('user_signature')), 
            axis=1
        )
        
        df_clean = df[~df['is_spam']].copy()
        after_filter = len(df_clean)
        
        print(f"è¿‡æ»¤å‰: {before_filter} æ¡")
        print(f"è¿‡æ»¤å: {after_filter} æ¡")
        print(f"è¿‡æ»¤æ‰: {before_filter - after_filter} æ¡åƒåœ¾è¯„è®º")
        
        return df_clean
    
    def clean_and_process_data(self, df):
        """æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†"""
        print("\n=== æ•°æ®æ¸…æ´—é˜¶æ®µ ===")
        
        # 1. æ—¶é—´æ ¼å¼è½¬æ¢
        print("1. æ—¶é—´æ ¼å¼è½¬æ¢...")
        df['create_time_dt'] = pd.to_datetime(df['create_time'], unit='s')
        df['create_time_str'] = df['create_time_dt'].dt.strftime('%Y-%m-%d %H:%M:%S')
        
        # 2. æ–‡æœ¬æ¸…æ´—
        print("2. æ–‡æœ¬æ¸…æ´—...")
        df['content_cleaned'] = df['content'].apply(self.clean_text)
        
        # 3. åˆ†è¯å¤„ç†
        print("3. åˆ†è¯å¤„ç†...")
        df['content_segmented'] = df['content_cleaned'].apply(self.segment_text)
        df['word_count'] = df['content_segmented'].apply(len)
        
        # 4. æ•°æ®ç»Ÿè®¡
        self.print_cleaning_stats(df)
        
        return df
    
    # ç§»é™¤æ—¶é—´å·®è®¡ç®—å‡½æ•°ï¼Œç”±æ—¶é—´åˆ†ææ¨¡å—è´Ÿè´£
    
    def print_cleaning_stats(self, df):
        """æ‰“å°æ¸…æ´—ç»Ÿè®¡ä¿¡æ¯"""
        print("\n=== æ¸…æ´—ç»Ÿè®¡ä¿¡æ¯ ===")
        print(f"æ¶‰åŠè§†é¢‘æ•°: {df['aweme_id'].nunique()}")
        print(f"æ¶‰åŠç”¨æˆ·æ•°: {df['user_id'].nunique()}")
        print(f"ä¸»è¯„è®ºæ•°: {len(df[df['parent_comment_id'].isnull() | (df['parent_comment_id'] == '0')])}")
        print(f"å­è¯„è®ºæ•°: {len(df[df['parent_comment_id'].notnull() & (df['parent_comment_id'] != '0')])}")
        
        if not df.empty:
            min_time = df['create_time_dt'].min()
            max_time = df['create_time_dt'].max()
            print(f"æ—¶é—´èŒƒå›´: {min_time} åˆ° {max_time}")
        
        # ç§»é™¤æ—¶é—´å·®ç»Ÿè®¡ï¼Œç”±æ—¶é—´åˆ†ææ¨¡å—è´Ÿè´£
    
    def analyze_data_distribution(self, conn, start_time=None, end_time=None):
        """åˆ†ææ•°æ®åº“ä¸­çš„æ•°æ®åˆ†å¸ƒ"""
        print("\n" + "="*50)
        print("=== æ•°æ®åº“æ•°æ®åˆ†å¸ƒåˆ†æ ===")
        print("="*50)
        
        # 1. æ€»ä½“æ•°æ®ç»Ÿè®¡
        print("\nğŸ“Š 1. æ€»ä½“æ•°æ®ç»Ÿè®¡")
        total_sql = "SELECT COUNT(*) as total FROM douyin_aweme_comment"
        total_count = pd.read_sql_query(total_sql, conn).iloc[0]['total']
        print(f"æ•°æ®åº“æ€»è¯„è®ºæ•°: {total_count:,} æ¡")
        
        # 2. æ—¶é—´åˆ†å¸ƒåˆ†æ
        print("\nğŸ“… 2. æ—¶é—´åˆ†å¸ƒåˆ†æ")
        time_sql = """
        SELECT 
            DATE(FROM_UNIXTIME(create_time)) as date,
            COUNT(*) as count
        FROM douyin_aweme_comment
        WHERE create_time IS NOT NULL
        GROUP BY DATE(FROM_UNIXTIME(create_time))
        ORDER BY date DESC
        LIMIT 10
        """
        time_df = pd.read_sql_query(time_sql, conn)
        print("æœ€è¿‘10å¤©è¯„è®ºåˆ†å¸ƒ:")
        for _, row in time_df.iterrows():
            print(f"  {row['date']}: {row['count']:,} æ¡")
        
        # 3. å†…å®¹é•¿åº¦åˆ†å¸ƒ
        print("\nğŸ“ 3. å†…å®¹é•¿åº¦åˆ†å¸ƒ")
        length_sql = """
        SELECT 
            CASE 
                WHEN CHAR_LENGTH(content) < 10 THEN '0-9å­—ç¬¦'
                WHEN CHAR_LENGTH(content) < 20 THEN '10-19å­—ç¬¦'
                WHEN CHAR_LENGTH(content) < 50 THEN '20-49å­—ç¬¦'
                WHEN CHAR_LENGTH(content) < 100 THEN '50-99å­—ç¬¦'
                ELSE '100+å­—ç¬¦'
            END as length_range,
            COUNT(*) as count
        FROM douyin_aweme_comment
        WHERE content IS NOT NULL
        GROUP BY length_range
        ORDER BY 
            CASE length_range
                WHEN '0-9å­—ç¬¦' THEN 1
                WHEN '10-19å­—ç¬¦' THEN 2
                WHEN '20-49å­—ç¬¦' THEN 3
                WHEN '50-99å­—ç¬¦' THEN 4
                ELSE 5
            END
        """
        length_df = pd.read_sql_query(length_sql, conn)
        print("å†…å®¹é•¿åº¦åˆ†å¸ƒ:")
        for _, row in length_df.iterrows():
            percentage = (row['count'] / total_count) * 100
            print(f"  {row['length_range']}: {row['count']:,} æ¡ ({percentage:.1f}%)")
        
        # 4. ç‚¹èµæ•°åˆ†å¸ƒ
        print("\nğŸ‘ 4. ç‚¹èµæ•°åˆ†å¸ƒ")
        likes_sql = """
        SELECT 
            CASE 
                WHEN CAST(like_count AS UNSIGNED) = 0 THEN '0ç‚¹èµ'
                WHEN CAST(like_count AS UNSIGNED) < 5 THEN '1-4ç‚¹èµ'
                WHEN CAST(like_count AS UNSIGNED) < 20 THEN '5-19ç‚¹èµ'
                WHEN CAST(like_count AS UNSIGNED) < 100 THEN '20-99ç‚¹èµ'
                ELSE '100+ç‚¹èµ'
            END as likes_range,
            COUNT(*) as count
        FROM douyin_aweme_comment
        WHERE like_count IS NOT NULL
        GROUP BY likes_range
        ORDER BY 
            CASE likes_range
                WHEN '0ç‚¹èµ' THEN 1
                WHEN '1-4ç‚¹èµ' THEN 2
                WHEN '5-19ç‚¹èµ' THEN 3
                WHEN '20-99ç‚¹èµ' THEN 4
                ELSE 5
            END
        """
        likes_df = pd.read_sql_query(likes_sql, conn)
        print("ç‚¹èµæ•°åˆ†å¸ƒ:")
        for _, row in likes_df.iterrows():
            percentage = (row['count'] / total_count) * 100
            print(f"  {row['likes_range']}: {row['count']:,} æ¡ ({percentage:.1f}%)")
        
        # 5. å­è¯„è®ºæ•°åˆ†å¸ƒ
        print("\nğŸ’¬ 5. å­è¯„è®ºæ•°åˆ†å¸ƒ")
        sub_comments_sql = """
        SELECT 
            CASE 
                WHEN CAST(sub_comment_count AS UNSIGNED) = 0 THEN '0å­è¯„è®º'
                WHEN CAST(sub_comment_count AS UNSIGNED) < 3 THEN '1-2å­è¯„è®º'
                WHEN CAST(sub_comment_count AS UNSIGNED) < 10 THEN '3-9å­è¯„è®º'
                WHEN CAST(sub_comment_count AS UNSIGNED) < 50 THEN '10-49å­è¯„è®º'
                ELSE '50+å­è¯„è®º'
            END as sub_comments_range,
            COUNT(*) as count
        FROM douyin_aweme_comment
        WHERE sub_comment_count IS NOT NULL
        GROUP BY sub_comments_range
        ORDER BY 
            CASE sub_comments_range
                WHEN '0å­è¯„è®º' THEN 1
                WHEN '1-2å­è¯„è®º' THEN 2
                WHEN '3-9å­è¯„è®º' THEN 3
                WHEN '10-49å­è¯„è®º' THEN 4
                ELSE 5
            END
        """
        sub_comments_df = pd.read_sql_query(sub_comments_sql, conn)
        print("å­è¯„è®ºæ•°åˆ†å¸ƒ:")
        for _, row in sub_comments_df.iterrows():
            percentage = (row['count'] / total_count) * 100
            print(f"  {row['sub_comments_range']}: {row['count']:,} æ¡ ({percentage:.1f}%)")
        
        # 6. ç©ºå†…å®¹ç»Ÿè®¡
        print("\nâŒ 6. æ•°æ®è´¨é‡é—®é¢˜")
        null_content_sql = "SELECT COUNT(*) as null_count FROM douyin_aweme_comment WHERE content IS NULL OR content = ''"
        null_count = pd.read_sql_query(null_content_sql, conn).iloc[0]['null_count']
        null_percentage = (null_count / total_count) * 100
        print(f"ç©ºå†…å®¹è¯„è®º: {null_count:,} æ¡ ({null_percentage:.1f}%)")
        
        # 7. çƒ­é—¨è§†é¢‘ç»Ÿè®¡
        print("\nğŸ”¥ 7. çƒ­é—¨è§†é¢‘ç»Ÿè®¡")
        hot_videos_sql = """
        SELECT 
            aweme_id,
            COUNT(*) as comment_count,
            AVG(CAST(like_count AS UNSIGNED)) as avg_likes,
            AVG(CAST(sub_comment_count AS UNSIGNED)) as avg_sub_comments
        FROM douyin_aweme_comment
        GROUP BY aweme_id
        HAVING comment_count >= 10
        ORDER BY comment_count DESC
        LIMIT 5
        """
        hot_videos_df = pd.read_sql_query(hot_videos_sql, conn)
        print("è¯„è®ºæ•°æœ€å¤šçš„5ä¸ªè§†é¢‘:")
        for _, row in hot_videos_df.iterrows():
            print(f"  è§†é¢‘ID {row['aweme_id']}: {row['comment_count']} æ¡è¯„è®º, å¹³å‡ç‚¹èµ {row['avg_likes']:.1f}, å¹³å‡å­è¯„è®º {row['avg_sub_comments']:.1f}")
    
    def analyze_filtering_effect(self, conn, df_original, df_filtered):
        """åˆ†æè¿‡æ»¤æ•ˆæœ"""
        print("\n" + "="*50)
        print("=== è¿‡æ»¤æ•ˆæœåˆ†æ ===")
        print("="*50)
        
        # è·å–æ•°æ®åº“ä¸­çš„åŸå§‹æ•°æ®ï¼ˆæœªç»è¿‡ä»»ä½•è¿‡æ»¤ï¼‰
        print("\nğŸ“Š è·å–åŸå§‹æ•°æ®åº“æ•°æ®è¿›è¡Œåˆ†æ...")
        original_sql = """
        SELECT 
            comment_id, aweme_id, parent_comment_id, content, create_time, 
            like_count, sub_comment_count, user_id, nickname, user_signature
        FROM douyin_aweme_comment
        WHERE content IS NOT NULL
        """
        df_db_original = pd.read_sql_query(original_sql, conn)
        
        total_db_original = len(df_db_original)
        total_original = len(df_original)  # SQLè¿‡æ»¤åçš„æ•°æ®
        total_filtered = len(df_filtered)  # åƒåœ¾è¯„è®ºè¿‡æ»¤åçš„æ•°æ®
        sql_filtered = total_db_original - total_original
        spam_filtered = total_original - total_filtered
        
        print(f"\nğŸ“Š æ€»ä½“è¿‡æ»¤æ•ˆæœ:")
        print(f"æ•°æ®åº“åŸå§‹æ•°æ®: {total_db_original:,} æ¡")
        print(f"SQLè¿‡æ»¤åæ•°æ®: {total_original:,} æ¡")
        print(f"åƒåœ¾è¯„è®ºè¿‡æ»¤å: {total_filtered:,} æ¡")
        print(f"SQLè¿‡æ»¤æ‰: {sql_filtered:,} æ¡")
        print(f"åƒåœ¾è¯„è®ºè¿‡æ»¤æ‰: {spam_filtered:,} æ¡")
        print(f"æ€»è¿‡æ»¤ç‡: {((sql_filtered + spam_filtered) / total_db_original * 100):.2f}%")
        
        # åˆ†æå„è¿‡æ»¤æ¡ä»¶çš„æ•ˆæœï¼ˆåŸºäºæ•°æ®åº“åŸå§‹æ•°æ®ï¼‰
        print(f"\nğŸ” å„è¿‡æ»¤æ¡ä»¶æ•ˆæœåˆ†æ:")
        
        # 1. å†…å®¹é•¿åº¦è¿‡æ»¤
        short_content = len(df_db_original[df_db_original['content'].str.len() < 5])
        short_percentage = (short_content / total_db_original) * 100
        print(f"1. å†…å®¹é•¿åº¦<5å­—ç¬¦: {short_content:,} æ¡ ({short_percentage:.2f}%)")
        
        # 2. ç‰¹æ®Šå­—ç¬¦è¿‡æ»¤
        special_char_filtered = 0
        for _, row in df_db_original.iterrows():
            content = row['content']
            if len(content) > 0:
                special_char_ratio = len(re.findall(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', content)) / len(content)
                if special_char_ratio > 0.6:
                    special_char_filtered += 1
        special_percentage = (special_char_filtered / total_db_original) * 100
        print(f"2. ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹>60%: {special_char_filtered:,} æ¡ ({special_percentage:.2f}%)")
        
        # 3. å¹¿å‘Šå…³é”®è¯è¿‡æ»¤
        ad_keywords = [
            'åŠ å¾®ä¿¡', 'åŠ qq', 'åŠ ç¾¤', 'ç§ä¿¡', 'è”ç³»æˆ‘', 'åˆä½œ', 'æ¨å¹¿', 'å¹¿å‘Š',
            'èµšé’±', 'å…¼èŒ', 'ä»£ç†', 'åŠ ç›Ÿ', 'æ‹›å•†', 'æŠ•èµ„', 'ç†è´¢', 'è´·æ¬¾',
            'å…è´¹é¢†å–', 'é™æ—¶ä¼˜æƒ ', 'ç‚¹å‡»é“¾æ¥', 'æ‰«ç å…³æ³¨', 'å…³æ³¨å…¬ä¼—å·'
        ]
        ad_filtered = 0
        for _, row in df_db_original.iterrows():
            content_lower = row['content'].lower()
            for keyword in ad_keywords:
                if keyword in content_lower:
                    ad_filtered += 1
                    break
        ad_percentage = (ad_filtered / total_db_original) * 100
        print(f"3. åŒ…å«å¹¿å‘Šå…³é”®è¯: {ad_filtered:,} æ¡ ({ad_percentage:.2f}%)")
        
        # 4. é‡å¤å­—ç¬¦è¿‡æ»¤
        repeated_filtered = 0
        for _, row in df_db_original.iterrows():
            content = row['content']
            if len(content) > 0 and len(set(content)) < len(content) * 0.2:
                repeated_filtered += 1
        repeated_percentage = (repeated_filtered / total_db_original) * 100
        print(f"4. é‡å¤å­—ç¬¦è¿‡å¤š: {repeated_filtered:,} æ¡ ({repeated_percentage:.2f}%)")
        
        # 5. ç”¨æˆ·ç­¾åè¿‡æ»¤
        signature_filtered = 0
        for _, row in df_db_original.iterrows():
            user_signature = row.get('user_signature')
            if user_signature:
                if len(user_signature) > 100 or re.search(r'[0-9]{8,}', user_signature):
                    signature_filtered += 1
        signature_percentage = (signature_filtered / total_db_original) * 100
        print(f"5. å¼‚å¸¸ç”¨æˆ·ç­¾å: {signature_filtered:,} æ¡ ({signature_percentage:.2f}%)")
        
        # æ•°æ®è´¨é‡æå‡ç»Ÿè®¡
        print(f"\nğŸ“ˆ æ•°æ®è´¨é‡æå‡:")
        if total_filtered > 0:
            # è®¡ç®—è¿‡æ»¤åçš„å¹³å‡å†…å®¹é•¿åº¦
            avg_length_original = df_db_original['content'].str.len().mean()
            avg_length_filtered = df_filtered['content'].str.len().mean()
            print(f"å¹³å‡å†…å®¹é•¿åº¦: {avg_length_original:.1f} â†’ {avg_length_filtered:.1f} å­—ç¬¦")
            
            # è®¡ç®—è¿‡æ»¤åçš„ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹
            special_chars_original = 0
            special_chars_filtered = 0
            for _, row in df_db_original.iterrows():
                content = row['content']
                special_chars_original += len(re.findall(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', content))
            for _, row in df_filtered.iterrows():
                content = row['content']
                special_chars_filtered += len(re.findall(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', content))
            
            total_chars_original = df_db_original['content'].str.len().sum()
            total_chars_filtered = df_filtered['content'].str.len().sum()
            
            if total_chars_original > 0 and total_chars_filtered > 0:
                special_ratio_original = (special_chars_original / total_chars_original) * 100
                special_ratio_filtered = (special_chars_filtered / total_chars_filtered) * 100
                print(f"ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹: {special_ratio_original:.2f}% â†’ {special_ratio_filtered:.2f}%")
    
    def save_processed_data(self, df, output_file):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        print(f"\n=== ä¿å­˜å¤„ç†ç»“æœ ===")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # é€‰æ‹©éœ€è¦ä¿å­˜çš„å­—æ®µ
        save_columns = [
            'comment_id', 'aweme_id', 'parent_comment_id', 
            'content', 'content_cleaned', 'content_segmented',
            'create_time', 'create_time_dt', 'create_time_str',
            'like_count', 'sub_comment_count', 'word_count'
        ]
        
        df_save = df[save_columns].copy()
        
        # ä¿å­˜ä¸ºJSON
        df_save.to_json(output_file, orient='records', force_ascii=False, indent=2)
        
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ“Š ä¿å­˜æ•°æ®é‡: {len(df_save)} æ¡")
        
        # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
        print("\n=== æ•°æ®é¢„è§ˆ ===")
        print(df_save.head(2).to_string())

def main():
    """ä¸»å‡½æ•°"""
    print("=== æŠ–éŸ³è¯„è®ºæ•°æ®å‡†å¤‡ä¸æ¸…æ´—å®Œæ•´æµç¨‹ ===")
    
    processor = CommentDataProcessor()
    
    try:
        # è·å–æ•°æ®åº“è¿æ¥
        conn = get_db_conn()
        print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        
        # è®¾ç½®è¿‡æ»¤æ¡ä»¶
        # é€‰é¡¹1ï¼šæœ€è¿‘90å¤©æ•°æ®
        # start_time = datetime.now() - timedelta(days=90)  # æœ€è¿‘90å¤©
        # end_time = datetime.now()
        
        # é€‰é¡¹2ï¼šå¤„ç†æ‰€æœ‰å†å²æ•°æ®ï¼ˆå–æ¶ˆæ³¨é‡Šä¸‹é¢çš„è¡Œï¼‰
        start_time = None  # ä¸é™åˆ¶å¼€å§‹æ—¶é—´
        end_time = None    # ä¸é™åˆ¶ç»“æŸæ—¶é—´
        
        print(f"æ—¶é—´èŒƒå›´: {start_time} åˆ° {end_time}")
        
        # ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®å‡†å¤‡ï¼ˆæå–åŸå§‹æ•°æ®ï¼‰
        print("\n" + "="*50)
        print("ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®å‡†å¤‡ï¼ˆæå–åŸå§‹æ•°æ®ï¼‰")
        print("="*50)
        
        df_raw = processor.get_raw_comments(
            conn=conn,
            start_time=start_time,
            end_time=end_time
        )
        
        if df_raw.empty:
            print("âŒ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
            return
        
        # ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®è¿‡æ»¤
        print("\n" + "="*50)
        print("ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®è¿‡æ»¤")
        print("="*50)
        
        df = processor.filter_spam_comments(df_raw)
        
        if df.empty:
            print("âŒ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
            return
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šæ•°æ®æ¸…æ´—
        print("\n" + "="*50)
        print("ç¬¬ä¸‰é˜¶æ®µï¼šæ•°æ®æ¸…æ´—")
        print("="*50)
        
        df_processed = processor.clean_and_process_data(df)
        
        # ç¬¬å››é˜¶æ®µï¼šæ•°æ®åˆ†å¸ƒåˆ†æ
        print("\n" + "="*50)
        print("ç¬¬å››é˜¶æ®µï¼šæ•°æ®åˆ†å¸ƒåˆ†æ")
        print("="*50)
        
        # åˆ†ææ•°æ®åº“ä¸­çš„æ•°æ®åˆ†å¸ƒ
        processor.analyze_data_distribution(conn, start_time, end_time)
        
        # åˆ†æè¿‡æ»¤æ•ˆæœ
        processor.analyze_filtering_effect(conn, df_raw, df_processed)
        
        # ä¿å­˜ç»“æœ
        output_file = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'douyin_comments_processed.json')
        processor.save_processed_data(df_processed, output_file)
        
        print("\nâœ… æ•°æ®å‡†å¤‡ã€æ¸…æ´—ä¸åˆ†æå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        if 'conn' in locals():
            conn.close()
            print("âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")

if __name__ == "__main__":
    main() 