# MediaCrawler 文本分析使用教程

## 📋 目录

1. [项目概述](#项目概述)
2. [环境准备](#环境准备)
3. [快速开始](#快速开始)
4. [详细使用指南](#详细使用指南)
5. [推荐分析流程](#推荐分析流程)
6. [参数说明](#参数说明)
7. [输出文件说明](#输出文件说明)
8. [常见问题](#常见问题)
9. [高级用法](#高级用法)

## 🎯 项目概述

MediaCrawler 文本分析模块是一个专注于从众心理分析的综合性工具集，提供以下核心功能：

- **数据清洗**：过滤垃圾评论，清洗文本数据
- **情感分析**：支持本地词典和阿里云API两种方式
- **时间分析**：分析评论时间分布，识别从众心理时间窗口
- **点赞分析**：分析点赞互动模式，识别意见领袖和社会认同信号

## 🔧 环境准备

### 1. Python环境

确保已安装Python 3.7+，推荐使用虚拟环境：

```bash
# 创建虚拟环境
python -m venv venv

# 激活虚拟环境
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate
```

### 2. 安装依赖

```bash
# 安装项目依赖
pip install -r requirements.txt

# 或使用uv（推荐）
uv sync
```

### 3. 数据库配置

确保MySQL数据库已配置，并在`config/db_config.py`中正确设置连接参数。

### 4. 阿里云API配置（可选）

如需使用阿里云情感分析API，请配置环境变量：

```bash
# 创建.env文件
cp text_analysis/env.example text_analysis/.env

# 编辑.env文件，添加阿里云API密钥
ALIYUN_ACCESS_KEY_ID=your_access_key_id
ALIYUN_ACCESS_KEY_SECRET=your_access_key_secret
```

## 🚀 快速开始

### 统一入口工具

项目提供了统一的命令行工具`text_analysis_unified.py`，支持所有分析模块：

```bash
# 查看帮助
python text_analysis_unified.py --help

# 查看各模块帮助
python text_analysis_unified.py cleaning --help
python text_analysis_unified.py sentiment --help
python text_analysis_unified.py time --help
python text_analysis_unified.py like --help
```

## 📖 详细使用指南

### 1. 数据清洗模块

数据清洗是分析流程的第一步，用于过滤垃圾评论和清洗文本数据。

#### 基本用法

```bash
# 清洗所有数据
python text_analysis_unified.py cleaning

# 清洗指定视频的评论
python text_analysis_unified.py cleaning --video-id 123456

# 测试模式（只处理少量数据）
python text_analysis_unified.py cleaning --test

# 限制处理数量
python text_analysis_unified.py cleaning --limit 1000
```

#### 参数说明

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `--video-id` | str | None | 指定视频ID，不指定则处理所有数据 |
| `--limit` | int | None | 限制处理数量 |
| `--test` | flag | False | 测试模式，只处理10条数据 |
| `--no-save` | flag | False | 不保存结果文件 |
| `--no-report` | flag | False | 不生成分析报告 |
| `--no-viz` | flag | False | 不创建可视化图表 |

### 2. 情感分析模块

支持本地词典和阿里云API两种分析方式。

#### 基本用法

```bash
# 使用本地词典分析（推荐用于测试）
python text_analysis_unified.py sentiment --use-cleaned-data --type local --test

# 使用阿里云API分析（推荐用于生产）
python text_analysis_unified.py sentiment --use-cleaned-data --type aliyun --test

# 分析指定视频的所有评论
python text_analysis_unified.py sentiment --use-cleaned-data --type local --video-id 123456

# 限制分析数量
python text_analysis_unified.py sentiment --use-cleaned-data --type local --limit 500
```

#### 参数说明

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `--use-cleaned-data` | flag | False | 使用清洗后的数据文件（推荐） |
| `--type` | choice | local | 分析器类型：local(本地词典) 或 aliyun(阿里云API) |
| `--video-id` | str | None | 指定视频ID，不指定则分析所有评论 |
| `--limit` | int | None | 限制分析数量 |
| `--cleaned-data-path` | str | None | 指定清洗数据文件路径 |
| `--test` | flag | False | 测试模式，只分析10条数据 |

#### 分析器对比

| 特性 | 本地词典 | 阿里云API |
|------|----------|-----------|
| 准确度 | 中等 | 高 |
| 速度 | 快 | 中等 |
| 网络依赖 | 无 | 需要 |
| 成本 | 免费 | 按量计费 |
| 适用场景 | 测试、离线 | 生产环境 |

### 3. 时间分析模块

分析评论时间分布，识别从众心理时间窗口。

#### 基本用法

```bash
# 使用清洗数据进行分析
python text_analysis_unified.py time --use-cleaned-data --test

# 分析指定视频
python text_analysis_unified.py time --use-cleaned-data --video-id 123456

# 限制分析数量
python text_analysis_unified.py time --use-cleaned-data --limit 1000
```

#### 参数说明

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `--use-cleaned-data` | flag | False | 使用清洗后的数据文件（推荐） |
| `--video-id` | str | None | 指定视频ID，不指定则分析所有数据 |
| `--limit` | int | None | 限制分析数量 |
| `--cleaned-data-path` | str | None | 指定清洗数据文件路径 |
| `--test` | flag | False | 测试模式，只分析10条数据 |

### 4. 点赞分析模块

分析点赞互动模式，识别意见领袖和社会认同信号。

#### 基本用法

```bash
# 使用清洗数据进行分析
python text_analysis_unified.py like --use-cleaned-data --test

# 分析指定视频
python text_analysis_unified.py like --use-cleaned-data --video-id 123456

# 限制分析数量
python text_analysis_unified.py like --use-cleaned-data --limit 1000
```

#### 参数说明

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `--use-cleaned-data` | flag | False | 使用清洗后的数据文件（推荐） |
| `--video-id` | str | None | 指定视频ID，不指定则分析所有数据 |
| `--limit` | int | None | 限制分析数量 |
| `--cleaned-data-path` | str | None | 指定清洗数据文件路径 |
| `--test` | flag | False | 测试模式，只分析10条数据 |

## 🔄 推荐分析流程

### 完整分析流程

1. **数据清洗**（必需）
   ```bash
   python text_analysis_unified.py cleaning --video-id 123456
   ```

2. **时间分析**（推荐）
   ```bash
   python text_analysis_unified.py time --use-cleaned-data --video-id 123456
   ```

3. **点赞分析**（推荐）
   ```bash
   python text_analysis_unified.py like --use-cleaned-data --video-id 123456
   ```

4. **情感分析**（推荐）
   ```bash
   # 使用本地词典（快速测试）
   python text_analysis_unified.py sentiment --use-cleaned-data --type local --video-id 123456
   
   # 使用阿里云API（高精度）
   python text_analysis_unified.py sentiment --use-cleaned-data --type aliyun --video-id 123456
   ```

### 测试流程

```bash
# 1. 测试数据清洗
python text_analysis_unified.py cleaning --test

# 2. 测试时间分析
python text_analysis_unified.py time --use-cleaned-data --test

# 3. 测试点赞分析
python text_analysis_unified.py like --use-cleaned-data --test

# 4. 测试情感分析
python text_analysis_unified.py sentiment --use-cleaned-data --type local --test
```

## 📊 输出文件说明

所有分析结果都保存在`MediaCrawler/data/`目录下，按模块和时间戳组织：

```
data/
├── processed/                    # 清洗后的数据
│   └── cleaned_data_YYYYMMDD_HHMMSS.json
├── results/                      # 分析结果
│   ├── cleaning_analysis_YYYYMMDD_HHMMSS.csv
│   ├── cleaning_analysis_YYYYMMDD_HHMMSS.json
│   ├── sentiment_analysis_YYYYMMDD_HHMMSS.csv
│   ├── sentiment_analysis_YYYYMMDD_HHMMSS.json
│   ├── time_analysis_YYYYMMDD_HHMMSS.csv
│   ├── time_analysis_YYYYMMDD_HHMMSS.json
│   ├── like_analysis_YYYYMMDD_HHMMSS.csv
│   └── like_analysis_YYYYMMDD_HHMMSS.json
├── reports/                      # 分析报告
│   ├── cleaning_analysis_report_YYYYMMDD_HHMMSS.json
│   ├── sentiment_analysis_report_YYYYMMDD_HHMMSS.json
│   ├── time_analysis_report_YYYYMMDD_HHMMSS.json
│   └── like_analysis_report_YYYYMMDD_HHMMSS.json
└── visualizations/               # 可视化图表
    ├── cleaning_analysis_main_YYYYMMDD_HHMMSS.png
    ├── sentiment_analysis_visualization_YYYYMMDD_HHMMSS.png
    ├── time_analysis_main_YYYYMMDD_HHMMSS.png
    └── like_analysis_main_YYYYMMDD_HHMMSS.png
```

### 文件命名规则

- **时间戳格式**：`YYYYMMDD_HHMMSS`
- **模块标识**：`cleaning`、`sentiment`、`time`、`like`
- **文件类型**：`.csv`（数据）、`.json`（结构化数据）、`.png`（图表）

## ❓ 常见问题

### Q1: 如何选择合适的分析器类型？

**A**: 
- **测试阶段**：使用`--type local`，无需网络，速度快
- **生产环境**：使用`--type aliyun`，准确度高，但需要配置API密钥

### Q2: 为什么推荐使用`--use-cleaned-data`？

**A**: 
- 清洗后的数据质量更高，分析结果更准确
- 避免重复清洗，提高分析效率
- 确保数据一致性

### Q3: 如何处理大量数据？

**A**: 
- 使用`--limit`参数限制处理数量
- 先用`--test`模式测试小批量数据
- 分批处理，避免内存溢出

### Q4: 如何自定义清洗数据路径？

**A**: 
```bash
python text_analysis_unified.py sentiment --use-cleaned-data --cleaned-data-path /path/to/your/cleaned_data.json
```

### Q5: 如何跳过某些输出？

**A**: 
```bash
# 只分析，不保存文件
python text_analysis_unified.py sentiment --use-cleaned-data --no-save --no-report --no-viz

# 只生成报告，不创建图表
python text_analysis_unified.py sentiment --use-cleaned-data --no-viz
```

## 🔧 高级用法

### 1. 批量处理多个视频

```bash
# 创建批处理脚本
for video_id in 123456 789012 345678; do
    echo "处理视频: $video_id"
    python text_analysis_unified.py cleaning --video-id $video_id
    python text_analysis_unified.py sentiment --use-cleaned-data --type local --video-id $video_id
done
```

### 2. 自定义分析参数

```bash
# 情感分析：限制数量，使用本地词典
python text_analysis_unified.py sentiment --use-cleaned-data --type local --limit 500 --video-id 123456

# 时间分析：指定清洗数据路径
python text_analysis_unified.py time --use-cleaned-data --cleaned-data-path data/processed/my_cleaned_data.json
```

### 3. 结果文件管理

```bash
# 查看最新的分析结果
ls -la data/results/ | tail -5

# 查看特定模块的结果
ls -la data/results/ | grep sentiment

# 清理旧的分析结果
find data/results/ -name "*.csv" -mtime +7 -delete
```

### 4. 环境变量配置

```bash
# 设置默认参数
export DEFAULT_VIDEO_ID=123456
export DEFAULT_ANALYZER_TYPE=local

# 在脚本中使用
python text_analysis_unified.py sentiment --use-cleaned-data --type $DEFAULT_ANALYZER_TYPE --video-id $DEFAULT_VIDEO_ID
```

## 📞 技术支持

如果在使用过程中遇到问题，请：

1. 查看本文档的常见问题部分
2. 检查控制台输出的错误信息
3. 确认环境配置是否正确
4. 查看各模块的详细算法文档

---

**最后更新**：2025-08-07  
**版本**：v2.0  
**作者**：CodyXie
